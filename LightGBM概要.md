# LightGBM(Light Gradient Boosting Machine)概要
教師あり学習  
決定木の手法の一つ  
決定木分析を1度実施するだけだと誤差が大きいため、複数回実施することで分類の精度を高める  
そのための代表的な方法が学習データを複数に分割する「バギング」とうい方法  
分割されたデータごとに決定木を行い、それぞれの結果を平均することで目的変数を推計する  
この方法は「ランダムフォレスト」と呼ばれる  
一方、LightGBMは「勾配ブースティング」という方法を用いて複数回の決定機分析を行う  

---
## 勾配ブースティング
---
「ブースティング」とは与えられたデータから決定木分析を行った後に、予測が正しくできなかったデータに重みをつけて、再度、決定木分析を行い、これを繰り返すことで精度を高める方法  
さらにデータに重みづけするのではなく、予測値と実績値の誤差を計算し、誤差を決定木で学習する方法が「勾配ブースティング」  
「勾配ブースティング」を用いたアルゴリズムとしては、XGBoost、Catboost、LightGBMなどがあり、LightGBMはデータ処理が非常に高速化(Light)されたことが特徴  

---
## LightGBMの特徴
---
「Gradient Boosting(勾配ブースティング)」を用いた決定木による機械学習の手法  
「勾配ブースティング」の場合は、誤差を最小化するように分割の要素、基準を見つけるため、データ量に応じて計算量が増える  
1つ1つの決定機の精度をなるべく落とさずに、高速に構築できるようにしたことが特徴  

---
### Leaf-wise tree growth
---
一般的な決定木の場合は、決定木の階層ごとに計算(Level wise)するため、1つの階層の分岐がすべて終わってから次の階層を計算するが、分岐が必要なくなった要素(=葉、leaf)については、それ以上は計算しない。

---
### Histogram based
---
決定木の分岐をする際に、すべての値をみるのではなく、ヒストグラムをつくって、数値をまとめて分岐させる。

---
### Gradient-based One-Side Sampling(GOSS)
---
学習できていない要素を学ぶことを優先するため、誤差が小さいデータは減らし、誤差の大きいデータだけを残すことで学習データの量を減らす。

---
### Exclusive Feature Bundling(EFB)
---
異なる特徴量の中でも、まとめても問題がなさそうな特徴量を1つにすることで計算量を減らす。
